---
title: "Quantitative Methods | Group 2"
author: "Frederic Denker & Nicolas GÃ¶ller"
date: "09.04.2020"
output: html_document
fontsize: 15pt
---


```{r setup, include=FALSE, echo=FALSE}
library(xts)
#library(TSA)
library(forecast)
library(tseries)
library(ggplot2)
library(rugarch)
library(stargazer)
library(gridExtra)
library(grid)
library(tidyverse)
library(lubridate)
library(DT)
library(ftplottools)



data <- read.csv("Presentation/DJI.csv")
dj_data <- readRDS("Presentation/Gtrends_DowJones.rds")
names(dj_data) <- c("date", "dj_hits", "dj_monrank", "dj_rank")
dji_data <- readRDS("Presentation/Gtrends_DowJonesIndustrial.rds")
names(dji_data) <- c("date", "dji_hits", "dji_monrank", "dji_rank")
djia_data <- readRDS("Presentation/Gtrends_DowJonesIndustrialAverage.rds")
names(djia_data) <- c("date", "djia_hits", "djia_monrank", "djia_rank")


gsv_data <- inner_join(dj_data, dji_data, by= "date")
gsv_data <- inner_join(gsv_data, djia_data, by= "date")
gsv_data$weekday <- wday(gsv_data$date)
gsv_data$weekday <- factor(gsv_data$weekday, c(2:7,1),  c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
#gsv_data <- na.omit(gsv_data)

dj_series <- ts(read.zoo(gsv_data[,c("date", "dj_rank")]))
dji_series <- ts(read.zoo(gsv_data[,c("date", "dji_rank")]))
djia_series <- ts(read.zoo(gsv_data[,c("date", "djia_rank")]))
dji_series_plot <- read.zoo(gsv_data[,c("date", "dji_rank")])
dji_diff_plot <- diff(dji_series_plot,7)


```
### Introduction

Our goal is to find out if Google trends and stock markets are related. However, to do this without using multivariate analysis we look into each of the time series separatly. We do this because demonstrating that each of the time series are random walks would give us the certainty that any correlation between the two is be spurious and no indicator for causality in the underlying process. This presentation is divided up into three parts, that will each try to answer a specific question:

* Are stock markets a random walk and can they be modeled?
* Are Google trends a random walk and can they be modeled? 
* What do we learn from this?

### Are stock markets a random walk?

This is an age-old question that has major implications for the financial industry. It is therefore not surprising that there is a whole range of literature that aims to answer this question. However, because this course is in quantitative methods and not in economic theory we will keep this summary very limited.

The Efficient market hypothesis (EMH) was formulated as a consequence of the inability of researchers in the 1950s to predict market price movements (Bodie et al.,2014). Instead of concluding market irrationality because of this, however, economists interpreted it as a result of efficient competition on the market. As any information that can be used for successful prediction should quickly be incorporated by the competing analysts, prices will be almost immediately driven back to their efficient level. Now, only unpredictable, new information will be able to move the market. It has to be new, as old information should already be accounted for and it also has to be unpredictable, as predictable (or in our case non-random) information would not count as new. As a result of this, the market should move in random, unpredictable changes, mathematically formulated as a random walk.

Building on this it is now the goal to evaluate the random walk hypothesis of stock movement by looking at the example of the *Dow Jones Industrial average*. To construct a model we follow the Box-Jenkins-Modelling philosophy:

However, to get an overview before we start, it might be helpful to plot this specific time series first:
```{r plot original, echo=FALSE, fig.width=12}
z <- read.zoo(data[,c(1,6)])


autoplot(z) +labs(title = "Dow Jones Industrial Average", y = "Adjusted Closing Price in USD", x = "") +  ft_theme()

```

We can see that stationarity is not given as even the mean is clearly dependent on time. To test whether we have unit roots, we are going to use the Augmented-Dickey-Fuller Test on the raw data:
```{r ADF, echo=FALSE}
adf_1 <- adf.test(z)
adf_1
```
For the Dickey-Fuller Test we use the regression $x_{t}=\rho x_{t-1}+u_t$ to test whether the sample $x_1,....,x_T$ has been generated by $X_{t}=X_{t-1}+u_t$ with $u_t \sim N(0,\sigma^2)$. The $H_0$ therefore is that $\rho = 1$. We cannot reject this $H_0$ as the p-value is larger than 0.05. 

In order to get rid of the unit root we take the log difference and plot this to get a better picture.

```{r pressure, echo=FALSE, fig.width=12}
lgx=diff(log(z))

autoplot(lgx) +labs(title = "Dow Jones Industrial Average", y = "Log Difference of Adjusted Closing Price", x = "") +  ft_theme()
```

This time series is then again tested for unit roots. Through conducting the ADF we are now able to reject the $H_0$ that $\rho =1$.

```{r ADF_2, echo=FALSE, warning=FALSE}
adf_2 <- adf.test(lgx)
adf_2
```

Next, we will try to find and model autocorrelation in the data. Below you will see both the ACF and the PACF visualized which should give us a first indication of what kind of model we should fit.

```{r ACF, echo=FALSE, fig.width=12}
lgx<- ts(lgx)
p1 <- ggAcf(lgx) + ggtitle("Dow Jones Industrial Average")+  ft_theme()
p2 <- ggPacf(lgx) + ggtitle("Dow Jones Industrial Average")+  ft_theme()

grid.arrange(p1,p2, ncol=2)

Box.test(lgx, type="Ljung-Box")

fit1 <- arma(lgx, c(0,  1 )) #MA(1)

fit2 <- arma(lgx, c(1,  0 )) #AR(1)

fit3 <- arma(lgx, c(1,  1 )) #ARMA(1,1)

fit4 <- arma(lgx, c(0,  2 ))

fit5 <- arma(lgx, c(2,  0 ))

fit6 <- arma(lgx, c(1,  2 )) #ARMA(2,2)

fit7 <- arma(lgx, c(2,  1 )) #ARMA(2,2)

fit8 <- arma(lgx, c(2,  2 )) #ARMA(2,2)
```

Looking at the ACF and PACF we concluded that there probably is a degree of autocorrelation (which is also confirmed when looking at the Ljung-Box Test), however it is challenging to extract specific information on the best ARMA process to model the underlying process. Therefore, we will look at the Akaike Information Criteria of a range of ARMA processes. Before this, however, through this autocorrelation we have an strong indication that we do not have a random walk.

```{r stargazer, echo=FALSE, results='asis', fig.align='center'}

model_names <- c("ARMA(0,1)","ARMA(1,0)", "ARMA(1,1)", "ARMA(0,2)", "ARMA(2,0)","ARMA(1,2)", "ARMA(2,1)", "ARMA(2,2)")
aic_values <- c(round(summary(fit1)$aic,1), round(summary(fit2)$aic,1), round(summary(fit3)$aic,1), round(summary(fit4)$aic,1),
               round(summary(fit5)$aic,1), round(summary(fit6)$aic,1), round(summary(fit7)$aic,1), round(summary(fit8)$aic,1) )
aic_table <- cbind(model_names, aic_values)
colnames(aic_table) <- c("Model specifications", "AIC")
stargazer(aic_table, column.labels =c("Model name","AIC value"), header=FALSE, type='html')

```

Looking at the summarized table, we can see that the ARMA(2,1) seems to fit our data best. Next, we will look at whether there is any autocorrelation left in the residuals. For this we use the Ljung-Box Test first on the residuals and then on the squared residuals.


```{r jbt, echo=FALSE}
Box.test(fit7$residuals, type="Ljung-Box")
Box.test((fit7$residuals)^2, type="Ljung-Box")
```

We are not able to reject the $H_0$ of the Ljung-Box Test on the residuals which means that there is no more autocorrelation left in the "normal" residuals. However, we can reject the $H_0$ for the squared residuals, which is a strong indication that there are ARCH effects present. Therefore, we will try to model this variance clustering (which we also observed earlier when looking at the Logged difference plot) through GARCH-models. 

```{r GARCH, echo=FALSE}
model=ugarchspec( variance.model = list(model = "sGARCH", garchOrder = c(2, 1)), 
                                    mean.model = list(armaOrder = c(2, 1), 
                                    include.mean = FALSE), distribution.model = "sstd" ) 
#Check other GARCH
modelfit=ugarchfit(spec=model,data=lgx)
#modelfit
## 0,2: AIC -6.1097
## 1,1: AIC -6.5624
## 1,0: AIC -6.3137
## 0,1: AIC -6.1101
## 2,1: AIC -6.5621
## 2,2: AIC -6.5619
#Box.test(fit6$residuals^2, type="Ljung-Box")

## std: 2,2: -6.6341
## sstd: 2.2: -6.6420
## ssted: 2,1: -6.6422
plot(modelfit,which=8)
#jarque.bera.test(modelfit@fit[["residuals"]])
```

However, as the Jarque-Bera Test and the plot show, our residuals are not normally distributed. Therefore, we selected the "standardized skew Student-t distribution" as it seems best fit the residuals. Lastly, we will quickly take a look at the GARCH model output:

```{r GARCH_output, echo=FALSE}
modelfit
```


This analysis has shown that we cannot reasonably assume the movements of the Dow Jones Industrial Average Index to follow a random walk. Instead we have found significant autocorrelation and volatility clustering that indicate for the data to have significant explanatory potential. In light of the Efficient Market Hypothesis, this implies the existence of market inefficiencies that keep the market from moving towards prices that account for all available information. For reasons of analytical scope, the potential causes of these inefficiencies cannot reasonably be discussed within this paper and but surely will be in subsequent research and the vibrant financial research community.


### Are Google trends a random walk and can they be modeled?

After looking at the time series for stock market movements in the US, we now turn to Google trends data and with that to one of the central assumptions of the efficient market hypothesis. Information has to be unpredictable. Google is by far the most popular search engine on the internet. Its prevalence in our world as a tool for information provision is widely known and several researchers have used data from Google trends to do research on different topics. Notable examples include research on epidemics and diseases (Carneiro & Mylonakis, 2009; Ginsberg et al., 2009). Applied to the context of financial market research, theoretically we would not expect any trends or regularities within the data, as this would indicate information to be significantly less random as initially assumed. Although the usage of google search volumes as predictive factors for analysis seems quite intuitive, things are not that simple. Below there is a plot of worldwide Google searches from 2010 until March 2020 for the term âDow Jonesâ.

```{r nico_plot_0, echo=FALSE, fig.width=12}
ggplot(data= gsv_data) + 
  geom_line(aes(x=date, y=dj_monrank*100), color= "red")+
  xlab("Date")+ylab("Search volume rank")+
  ft_theme()
```

One specialty about Google trends data is that each query for data will be normalized on a scale of 0 to 100 with 100 signifying the point in time with the highest search volume (for the selected time period and the region of occurrence). Thus, it is not possible to access absolute search volume numbers. Additionally, it is not possible to pick the frequency of reporting for the trends, as it will automatically adapt to the length of the time period that is queried. We attempted to deal with these problems by querying daily results for each month of our time of observation and calculate daily results for the whole period by combining with the monthly value. The results can be seen below:

```{r nico_plot_2, echo=FALSE, results='asis'}
overview_gsv<- head(gsv_data[,c("date","dj_hits","dj_monrank","dj_rank")]) %>% as.matrix()
#stargazer(overview_gsv, header=FALSE, type='html')
datatable(gsv_data[,c("date","dj_hits","dj_monrank","dj_rank")], rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T) )
```
```{r nico_plot_3, echo=FALSE, fig.width=12}
ggplot(data= gsv_data) + 
  geom_line(aes(x=date, y=dj_rank), color= "black")+
  geom_line(aes(x=date, y=dj_monrank*100), color= "red")+
  xlab("Date")+ylab("Search volume rank")+
  ft_theme()
```
Still, these characteristics of the data make it impossible to describe the data as a linear trend series as it will be necessarily limited to 100 on its focal point. Linear series would thus yield impossible results and cannot be interpreted. It is therefore important not to use the following models for forecasting, but only for explanatory analysis (including our question of the series being a random walk). We pulled the data for three different search term âDow Jonesâ in red, âDow Jones Industrialâ in black and âDow Jones Industrial Averageâ in blue. We decided to proceed with the analysis exclusively on the last search term as it had the highest search results in total. To construct a model we follow the Box-Jenkins-Modelling philosophy.

```{r nico_plot_1, echo=FALSE, fig.width=12}
ggplot(data= gsv_data) + 
  geom_line(aes(x=date, y=dj_monrank*100), color= "red")+
  geom_line(aes(x=date, y=dji_monrank*100), color= "black")+
  geom_line(aes(x=date, y=djia_monrank*100), color= "blue")+
  xlab("Date")+ylab("Monthly search volume rank")+
  ft_theme()
```

We computed tests for normality and stationarity which allowed us to proceed but we do not report them here as they will be obsolete in a second. A look at the ACF and PACF revealed some interesting facts about our data. 

```{r nico_plot_4, echo=FALSE, fig.width=12}
p1 <- ggAcf(djia_series) + ggtitle("DJIA | Search Rank")+  ft_theme()
p2 <- ggPacf(djia_series) + ggtitle("DJIA | Search Rank")+  ft_theme()

grid.arrange(p1,p2, ncol=2)

```

There is an obvious seasonality feature to be seen here, specifically for a lag of seven days and we were able to confirm this by looking at a boxplot for the different weekdays. Theoretically, for the purpose of answering the question of whether the Google trend follow a random walk, we could stop our analysis here. However, continuing the analysis has the potential to show even more dimensions of non-randomness in the series.

```{r nico_plot_5, echo=FALSE, fig.width=12}
djia_diff <- diff(djia_series, 7)
ggplot(gsv_data, aes(x= weekday, y=djia_rank))+
  geom_boxplot()+
  xlab("Weekday")+ylab("Search volume distribution")+
  ft_theme()

```

After including a forward difference of 7 days in the data, the new series shows a much-reduced seasonality pattern. The test for normality and stationarity conducted now yield results of stationary data that is however not normally distributed (as it is usually done we ignore this due to time constraints and use ML estimation).

```{r nico_plot_6, echo=FALSE, fig.width=12}
##Here noch series
p0<- autoplot(dji_diff_plot) +labs(title = "DJIA-Diff(7) | Search Rank Differences", y = "", x = "") +  ft_theme()
p1 <- ggAcf(djia_diff) + ggtitle("DJIA-Diff(7) | Search Rank")+  ft_theme()
p2 <- ggPacf(djia_diff) + ggtitle("DJIA-Diff(7) | Search Rank")+  ft_theme()
p0
grid.arrange(p1,p2, ncol=2)

```

Fitting several ARMA models provided us with the following result and the best model under the AIC criteria in ARMA(3,4).


```{r nico_plot_7, echo=FALSE, results='asis'}
nfit1 <- arma(djia_diff, c(0,1))
nfit2 <- arma(djia_diff, c(1,0))
nfit3 <- arma(djia_diff, c(1,1))
nfit4 <- arma(djia_diff, c(0,2))
nfit5 <- arma(djia_diff, c(2,0))
nfit6 <- arma(djia_diff, c(1,2))
nfit7 <- arma(djia_diff, c(2,1))
nfit8 <- arma(djia_diff, c(2,2))
nfit9 <- arma(djia_diff, c(3,2))
nfit10 <- arma(djia_diff, c(2,3))
nfit11 <- arma(djia_diff, c(3,3))
nfit12 <- arma(djia_diff, c(3,4)) #Best fit

model_names <- c("ARMA(0,1)","ARMA(1,0)", "ARMA(1,1)", "ARMA(0,2)", "ARMA(2,0)","ARMA(1,2)", "ARMA(2,1)", "ARMA(2,2)",
                 "ARMA(3,2)", "ARMA(2,3)", "ARMA(3,3)", "ARMA(3,4)")
aic_values <- c(round(summary(nfit1)$aic,1), round(summary(nfit2)$aic,1), round(summary(nfit3)$aic,1), round(summary(nfit4)$aic,1),
                round(summary(nfit5)$aic,1), round(summary(nfit6)$aic,1), round(summary(nfit7)$aic,1), round(summary(nfit8)$aic,1),
                round(summary(nfit9)$aic,1), round(summary(nfit10)$aic,1), round(summary(nfit11)$aic,1), round(summary(nfit12)$aic,1))
aic_table <- cbind(model_names, aic_values)
colnames(aic_table) <- c("Model specifications", "AIC")
stargazer(aic_table, column.labels =c("Model name","AIC value"), header=FALSE, type='html')

```

The Box-Ljung test on the residuals of the final model, gave no indication for autocorrelation in the regular residuals. However, the test on the squared residuals show the existence of significant ARCH effects in the series.

```{r nico_plot_8, echo=FALSE}
##Test for resid autocorr
Box.test(nfit12$residuals, type = "Ljung-Box")

## test for ARCH effects
Box.test((nfit12$residuals)^2, type = "Ljung-Box")
```

Therefore, we conducted a GARCH-model on the specifications of the ARMA models from above:

```{r nico_plot_9, echo=FALSE, fig.width=12}
model=ugarchspec( variance.model = list(model = "sGARCH", garchOrder = c(1, 1)), 
                  mean.model = list(armaOrder = c(3, 4), 
                                    include.mean = FALSE), distribution.model = "sstd" ) 
#Check other GARCH
modelfit=ugarchfit(spec=model,data=djia_diff)
modelfit
plot(modelfit,which=8)
```

These show that we have highly significant coefficients which are strong indicators that there is volatility clustering present.

In conclusion we can say that we can clearly reject the notion of Google search volume trends being a random walk (at least for the term âDow Jones Industrial Averageâ). It seems to be the case that this series holds a lot of explanatory power that could be potentially be used to explain and predict movements of its real-world object. This is very much in line with previous studies on the subject that showed impressive predictive results for stock movements by incorporating several search terms from Google trends into their analysis. It also indicates that it should be closely observed, how and when information about financial news is attained and processes to gain a deeper understanding of the causal relations of stock market movements and investor behavior.

### What do we learn from this?

Within this analysis, we have discussed time series analysis for both stock movements of the Dow Jones Industrial Average and Google Search Volumes on this term. We were able to provide serious evidence against the hypothesis of the stock movement following a random walk. We were also able to show that there are relevant regularities and other, yet unexplained non-random factors that determine search volumes of the Dow Jones Index on Google.
Despite all this evidence, it is not possible to make a statement about causal relations between the two data series as this would mandate a the use of multivariate time series analysis. Such research has previously been done by other researchers (Bijl et al., 2016; Challet & Bel Hadj Ayed, 2013; Preis et al., 2010) and results have been mixed. While this might be partly due to data quality issues, both the foundational issues put on the table by our analysis and the results of the other researchers point to the necessity of further research into the topic. It seems to be the case that current financial market theory does not realistically describe the way how people access and process information and how this might affect the movements of the market. 


#### Bibliography

Bijl, L., Kringhaug, G., MolnÃ¡r, P., & Sandvik, E. (2016). Google searches and stock returns. International Review of Financial Analysis, 45(March), 150â156. https://doi.org/10.1016/j.irfa.2016.03.015

Bodie, Z., Kane, A., & Marcus, A. (2014). Investments (10th global ed.). Berkshire: McGraw-Hill Education.

Carneiro, H. A., & Mylonakis, E. (2009). Google Trends: A WebâBased Tool for RealâTime Surveillance of Disease Outbreaks. Clinical Infectious Diseases, 49(10), 1557â1564. https://doi.org/10.1086/630200

Challet, D., & Bel Hadj Ayed, A. (2013). Predicting Financial Markets with Google Trends and Not so Random Keywords. SSRN Electronic Journal, July 2013. https://doi.org/10.2139/ssrn.2310621

Ginsberg, J., Mohebbi, M. H., Patel, R. S., Brammer, L., Smolinski, M. S., & Brilliant, L. (2009). Detecting influenza epidemics using search engine query data. Nature, 457(7232), 1012â1014. https://doi.org/10.1038/nature07634

Preis, T., Moat, H. S., & Eugene Stanley, H. (2013). Quantifying trading behavior in financial markets using google trends. Scientific Reports, 3(April). https://doi.org/10.1038/srep01684

Preis, T., Reith, D., & Stanley, H. E. (2010). Complex dynamics of our economic life on different scales: Insights from search engine query data. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 368(1933), 5707â5719. https://doi.org/10.1098/rsta.2010.0284


